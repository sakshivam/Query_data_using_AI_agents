{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Install and import Necessary Libraries\n",
    "You need to install the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain_community langchain_huggingface faiss-cpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Extract Text from the PDF Document\n",
    "We can use PyPDF to extract text from a PDF. You can parse the PDF and split it into chunks for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF document and loads it into LangChain's document structure.\n",
    "    :param pdf_path: Path to the PDF file\n",
    "    :return: List of LangChain Document objects\n",
    "    \"\"\"\n",
    "    loader = PyPDFLoader(file_path=pdf_path)\n",
    "    docs = loader.load()\n",
    "    return docs\n",
    "# docs = extract_text_from_pdf(\"../data/Responsible_data_sharing.pdf\")\n",
    "# print(len(docs))\n",
    "# print(f\"{docs[0].page_content[:200]}\\n\")\n",
    "# print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def split_texts_to_chunks(\n",
    "\n",
    "    docs: List[Document], chunk_size: int = 1000, chunk_overlap: int = 200\n",
    "\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Splits the text from documents into smaller chunks for processing.\n",
    "\n",
    "    :param docs: List of LangChain Document objects\n",
    "\n",
    "    :param chunk_size: Size of each chunk\n",
    "\n",
    "    :param chunk_overlap: Overlap between consecutive chunks\n",
    "\n",
    "    :return: List of split documents\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, add_start_index=True\n",
    "\n",
    "    )\n",
    "\n",
    "    all_splits = text_splitter.split_documents(docs)\n",
    "    return all_splits\n",
    "\n",
    "\n",
    "# all_splits = split_texts_to_chunks(docs)\n",
    "\n",
    "# len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Embed the Text Using Hugging Face Transformers and store them in FAISS vectorstore\n",
    "We will use Hugging Face's transformer model (e.g., sentence-transformers/all-MiniLM-L6-v2) to convert the text into embeddings.FAISS is used for efficient similarity search in high-dimensional spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import faiss\n",
    "\n",
    "def create_vector_store(all_splits: List[Document], model_name: str):\n",
    "    \"\"\"\n",
    "    Creates a vector store from document chunks.\n",
    "    :param all_splits: List of split LangChain Document objects\n",
    "    :param model_name: Name of the embedding model\n",
    "    :return: Initialized FAISS vector store\n",
    "    \"\"\"\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    sample_vector = embeddings.embed_query(all_splits[0].page_content)\n",
    "    index = faiss.IndexFlatL2(len(sample_vector))\n",
    "    vector_store = FAISS(\n",
    "        embedding_function=embeddings,\n",
    "        index=index,\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "    )\n",
    "    vector_store.add_documents(documents=all_splits)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Create a RAG-based Retrieval System with Langchain\n",
    "Langchain helps in creating complex chains of logic for RAG systems. It integrates both the document retrieval and generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_documents(vector_store, queries: List[str], k: int = 1):\n",
    "    \"\"\"\n",
    "    Retrieves the most similar documents for each query.\n",
    "    :param vector_store: FAISS vector store\n",
    "    :param queries: List of queries\n",
    "    :param k: Number of results to retrieve\n",
    "    :return: List of retrieved documents\n",
    "    \"\"\"\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": k},\n",
    "    )\n",
    "    results = retriever.batch(queries)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Putting It All Together\n",
    "Now, you can combine everything into a complete pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    pdf_path: str,\n",
    "    queries: List[str],\n",
    "    model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Main function to extract text from a PDF, split into chunks, create a vector store, and retrieve documents.\n",
    "    :param pdf_path: Path to the PDF file\n",
    "    :param queries: List of queries\n",
    "    :param model_name: Embedding model name\n",
    "    \"\"\"\n",
    "    # Step 1: Extract text from PDF\n",
    "    docs = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    # Step 2: Split documents into smaller chunks\n",
    "    all_splits = split_texts_to_chunks(docs)\n",
    "\n",
    "    # Step 3: Create a vector store using embeddings\n",
    "    vector_store = create_vector_store(all_splits, model_name)\n",
    "\n",
    "    # Step 4: Retrieve documents based on queries\n",
    "    results = retrieve_documents(vector_store, queries)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saksh\\SakshiOneDrive\\OneDrive\\SakshiOnedrive\\Projects\\20250111_Query_data_using_AIAgents\\ai_agent\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: give the summary of this document\n",
      "Document: request the information required to meet the specified purpose for which it is being requested and \n",
      "should indicate a timeline for destruction of the data. Humanitarian organisations should document \n",
      "all requests for data and ensure consistency in responding to these requests over time. \n",
      "• Investing in data management capacities of staff and organizations\n",
      "Donors and humanitarian organizations should identify opportunities to invest in building data \n",
      "management expertise especially for non-technical staff. The donor community is uniquely positioned \n",
      "to encourage data responsibility by  providing additional  resources for training and capacity building. \n",
      "• Adopting common principles for donor data management\n",
      "The sector already has a range of principles and commitments to inform different aspects of \n",
      "humanitarian donorship.15 However, these do not sufficiently address concerns related to data \n",
      "responsibility. Donors and partners should engage in the development of common principles and\n",
      "\n",
      "Query: what is the meaning of responsible data sharing?\n",
      "Document: Note series follows the publication of the OCHA Data Responsibility Guidelines in March 2019. Through \n",
      "the series, the Centre aims to provide additional guidance on specific issues, processes and tools for data \n",
      "responsibility in practice. This series is made possible with the generous support of the Directorate-General for \n",
      "European Civil Protection and Humanitarian Aid Operations (DG ECHO).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"../data/Responsible_data_sharing.pdf\"\n",
    "    queries = [\n",
    "        \"give the summary of this document\",\n",
    "        \"what is the meaning of responsible data sharing?\",\n",
    "    ]\n",
    "    retrieved_docs = main(pdf_path, queries)\n",
    "\n",
    "    for query, docs in zip(queries, retrieved_docs):\n",
    "        print(f\"Query: {query}\")\n",
    "        for doc in docs:\n",
    "            print(f\"Document: {doc.page_content}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
